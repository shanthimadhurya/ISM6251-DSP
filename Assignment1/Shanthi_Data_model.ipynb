{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d496d733-0cdc-4d33-8fdb-b98125637c1d",
   "metadata": {},
   "source": [
    "## Data modeling using generated data file after performing data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a099445-d335-4d07-aaee-8047b210ec17",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4774cc6-cf24-4dc0-b166-82bb6cdb53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099a185-81cf-49da-9a62-d27893a1b767",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07938061-9bca-4675-8f3a-cf3b76a644e8",
   "metadata": {},
   "source": [
    "X_train = pd.read_csv(\"appledata_train_X.csv\")\n",
    "X_test = pd.read_csv(\"appledata_test_X.csv\")\n",
    "y_train = pd.read_csv(\"appledata_train_y.csv\")\n",
    "y_test = pd.read_csv(\"appledata_test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5f6b8-b67b-4dab-80b0-a9580e385bb2",
   "metadata": {},
   "source": [
    "## 3. creating performance data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a43e12b3-0270-4c0f-9b93-1a8c286588b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c7611-a45a-4e04-9d1a-3fa0eb1d2620",
   "metadata": {},
   "source": [
    "## 4. Logistic regression using Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4080a4c-b44d-4b79-ad11-615eed057270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.8397435897435898\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1255 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "645 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "270 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.83974359        nan        nan        nan\n",
      " 0.83974359 0.82435897 0.82435897        nan        nan        nan\n",
      " 0.83974359        nan 0.82435897 0.83974359 0.82435897        nan\n",
      "        nan        nan        nan 0.83974359 0.82435897 0.82435897\n",
      " 0.82435897        nan        nan        nan 0.82435897        nan\n",
      " 0.82435897 0.82435897 0.82435897        nan        nan        nan\n",
      "        nan        nan 0.83974359 0.82435897 0.82435897        nan\n",
      " 0.82435897        nan        nan        nan        nan        nan\n",
      " 0.83974359 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897        nan 0.82435897        nan        nan 0.82435897\n",
      " 0.82435897 0.83974359 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.83974359 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897 0.82435897        nan        nan\n",
      " 0.83974359 0.83974359        nan 0.83974359 0.82435897 0.83974359\n",
      " 0.82435897 0.82435897 0.83974359 0.82435897 0.82435897 0.83974359\n",
      " 0.82435897        nan 0.83974359 0.83974359        nan        nan\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897 0.82435897        nan        nan 0.82435897 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.82435897        nan        nan\n",
      "        nan 0.82435897 0.82435897        nan        nan 0.82435897\n",
      " 0.82435897 0.83974359        nan 0.82435897 0.82435897        nan\n",
      "        nan 0.82435897 0.82435897        nan        nan 0.83974359\n",
      " 0.82435897        nan        nan        nan 0.83974359        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82435897        nan 0.82435897        nan 0.82435897        nan\n",
      "        nan 0.82435897        nan        nan 0.82435897        nan\n",
      " 0.83974359 0.83974359        nan        nan        nan 0.82435897\n",
      "        nan        nan 0.82435897        nan        nan 0.83974359\n",
      " 0.82435897 0.83974359        nan 0.82435897 0.82435897 0.82435897\n",
      "        nan 0.82435897        nan 0.82435897        nan        nan\n",
      "        nan 0.82435897        nan        nan 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82435897 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897 0.82435897        nan 0.82435897 0.82435897\n",
      " 0.83974359 0.82435897 0.82435897        nan 0.83974359        nan\n",
      "        nan        nan 0.82435897 0.83974359 0.82435897 0.82435897\n",
      " 0.82435897        nan        nan 0.82435897        nan        nan\n",
      "        nan 0.82435897        nan        nan        nan 0.82435897\n",
      "        nan 0.82435897 0.82435897        nan 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan 0.82435897 0.82435897\n",
      " 0.82435897 0.82435897        nan 0.82435897 0.82435897 0.82435897\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897 0.83974359 0.82435897        nan        nan        nan\n",
      "        nan 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897        nan        nan        nan        nan 0.82435897\n",
      "        nan 0.82435897        nan        nan 0.82435897        nan\n",
      " 0.82435897        nan 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897        nan 0.82435897 0.83974359\n",
      " 0.82435897        nan 0.83974359 0.82435897        nan 0.82435897\n",
      "        nan        nan 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897        nan 0.83974359        nan        nan\n",
      "        nan        nan 0.82435897 0.82435897        nan        nan\n",
      "        nan 0.83974359        nan        nan 0.82435897 0.82435897\n",
      "        nan        nan 0.82435897        nan 0.83974359        nan\n",
      " 0.83974359        nan        nan 0.83974359 0.83974359 0.82435897\n",
      "        nan        nan        nan 0.82435897 0.82435897        nan\n",
      "        nan 0.82435897 0.83974359        nan 0.82435897        nan\n",
      "        nan        nan        nan 0.82435897        nan 0.83974359\n",
      "        nan 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897        nan        nan 0.83974359        nan 0.82435897\n",
      " 0.82435897        nan        nan        nan 0.83974359 0.83974359\n",
      " 0.83974359 0.82435897        nan        nan        nan        nan\n",
      " 0.83974359 0.82435897        nan        nan 0.82435897 0.82435897\n",
      "        nan 0.82435897        nan        nan 0.82435897 0.83974359\n",
      "        nan        nan        nan 0.82435897        nan 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.83974359        nan 0.82435897\n",
      "        nan 0.82435897        nan 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897 0.83974359 0.83974359        nan 0.82435897\n",
      " 0.83974359        nan        nan        nan 0.82435897 0.82435897\n",
      "        nan 0.83974359 0.82435897 0.82435897 0.83974359 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.83974359 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan 0.83974359        nan\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897        nan        nan        nan 0.82435897\n",
      " 0.82435897 0.82435897        nan 0.82435897        nan 0.82435897\n",
      "        nan        nan        nan 0.83974359 0.82435897 0.82435897\n",
      "        nan        nan        nan 0.83974359        nan 0.82435897\n",
      "        nan        nan 0.82435897 0.83974359 0.82435897 0.83974359\n",
      " 0.82435897        nan        nan        nan        nan 0.82435897\n",
      " 0.83974359 0.83974359 0.83974359        nan        nan        nan\n",
      "        nan        nan 0.82435897        nan        nan        nan\n",
      " 0.82435897        nan        nan 0.83974359 0.83974359        nan\n",
      " 0.83974359 0.82435897]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.90734694        nan        nan        nan\n",
      " 0.90734694 0.90326531 0.91134694        nan        nan        nan\n",
      " 0.90734694        nan 0.90326531 0.90734694 0.91134694        nan\n",
      "        nan        nan        nan 0.90734694 0.91134694 0.90326531\n",
      " 0.90326531        nan        nan        nan 0.90326531        nan\n",
      " 0.91134694 0.90326531 0.90326531        nan        nan        nan\n",
      "        nan        nan 0.90734694 0.91134694 0.91134694        nan\n",
      " 0.90326531        nan        nan        nan        nan        nan\n",
      " 0.90734694 0.90326531        nan        nan        nan        nan\n",
      " 0.90326531 0.90326531 0.91134694 0.90326531        nan 0.91134694\n",
      " 0.90326531        nan 0.90326531        nan        nan 0.90326531\n",
      " 0.90326531 0.90734694 0.91134694 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694 0.90734694 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694 0.90326531 0.90326531        nan        nan\n",
      " 0.90734694 0.90734694        nan 0.90734694 0.90326531 0.90734694\n",
      " 0.90326531 0.90326531 0.90734694 0.91134694 0.90326531 0.90734694\n",
      " 0.90326531        nan 0.90734694 0.90734694        nan        nan\n",
      "        nan 0.90326531 0.91134694 0.90326531        nan 0.90326531\n",
      " 0.90326531 0.90326531        nan        nan 0.90326531 0.90326531\n",
      " 0.90326531        nan 0.90326531 0.90326531        nan        nan\n",
      "        nan 0.91134694 0.90326531        nan        nan 0.91134694\n",
      " 0.90326531 0.90734694        nan 0.90326531 0.90326531        nan\n",
      "        nan 0.91134694 0.90326531        nan        nan 0.90734694\n",
      " 0.90326531        nan        nan        nan 0.90734694        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.90326531        nan 0.90326531        nan 0.90326531        nan\n",
      "        nan 0.90326531        nan        nan 0.91134694        nan\n",
      " 0.90734694 0.90734694        nan        nan        nan 0.91134694\n",
      "        nan        nan 0.90326531        nan        nan 0.90734694\n",
      " 0.90326531 0.90734694        nan 0.90326531 0.90326531 0.91134694\n",
      "        nan 0.90326531        nan 0.90326531        nan        nan\n",
      "        nan 0.90326531        nan        nan 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.90326531 0.90326531        nan 0.90326531\n",
      "        nan 0.90326531 0.91134694        nan 0.90326531 0.90326531\n",
      " 0.90734694 0.91134694 0.91134694        nan 0.90734694        nan\n",
      "        nan        nan 0.90326531 0.90734694 0.90326531 0.91134694\n",
      " 0.90326531        nan        nan 0.90326531        nan        nan\n",
      "        nan 0.90326531        nan        nan        nan 0.91134694\n",
      "        nan 0.91134694 0.90326531        nan 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan 0.91134694 0.91134694\n",
      " 0.90326531 0.90326531        nan 0.90326531 0.90326531 0.90326531\n",
      "        nan 0.90326531 0.91134694 0.90326531        nan 0.91134694\n",
      " 0.90326531 0.90734694 0.91134694        nan        nan        nan\n",
      "        nan 0.90326531        nan        nan        nan        nan\n",
      " 0.90326531        nan        nan        nan        nan 0.90326531\n",
      "        nan 0.90326531        nan        nan 0.90326531        nan\n",
      " 0.91134694        nan 0.90326531 0.90326531        nan        nan\n",
      " 0.91134694 0.91134694 0.90326531        nan 0.90326531 0.90734694\n",
      " 0.91134694        nan 0.90734694 0.90326531        nan 0.90326531\n",
      "        nan        nan 0.90326531 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694        nan 0.90734694        nan        nan\n",
      "        nan        nan 0.90326531 0.90326531        nan        nan\n",
      "        nan 0.90734694        nan        nan 0.90326531 0.90326531\n",
      "        nan        nan 0.90326531        nan 0.90734694        nan\n",
      " 0.90734694        nan        nan 0.90734694 0.90734694 0.91134694\n",
      "        nan        nan        nan 0.90326531 0.90326531        nan\n",
      "        nan 0.90326531 0.90734694        nan 0.90326531        nan\n",
      "        nan        nan        nan 0.90326531        nan 0.90734694\n",
      "        nan 0.90326531        nan        nan        nan        nan\n",
      " 0.91134694        nan        nan 0.90734694        nan 0.90326531\n",
      " 0.91134694        nan        nan        nan 0.90734694 0.90734694\n",
      " 0.90734694 0.90326531        nan        nan        nan        nan\n",
      " 0.90734694 0.91134694        nan        nan 0.90326531 0.90326531\n",
      "        nan 0.90326531        nan        nan 0.90326531 0.90734694\n",
      "        nan        nan        nan 0.90326531        nan 0.90326531\n",
      " 0.90326531        nan 0.90326531 0.90734694        nan 0.91134694\n",
      "        nan 0.90326531        nan 0.91134694        nan 0.90326531\n",
      "        nan 0.90326531 0.90734694 0.90734694        nan 0.90326531\n",
      " 0.90734694        nan        nan        nan 0.90326531 0.91134694\n",
      "        nan 0.90734694 0.90326531 0.90326531 0.90734694 0.90326531\n",
      " 0.91134694        nan 0.90326531 0.90734694 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan 0.90734694        nan\n",
      "        nan 0.90326531 0.90326531 0.90326531        nan 0.91134694\n",
      "        nan 0.90326531        nan        nan        nan 0.90326531\n",
      " 0.90326531 0.90326531        nan 0.91134694        nan 0.90326531\n",
      "        nan        nan        nan 0.90734694 0.91134694 0.90326531\n",
      "        nan        nan        nan 0.90734694        nan 0.90326531\n",
      "        nan        nan 0.90326531 0.90734694 0.91134694 0.90734694\n",
      " 0.90326531        nan        nan        nan        nan 0.90326531\n",
      " 0.90734694 0.90734694 0.90734694        nan        nan        nan\n",
      "        nan        nan 0.91134694        nan        nan        nan\n",
      " 0.90326531        nan        nan 0.90734694 0.90734694        nan\n",
      " 0.90734694 0.91134694]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter':np.arange(500,1000),\n",
    "    'penalty': ['None','l1','l2','elasticnet'],\n",
    "    'solver':['saga','liblinear']\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = log_reg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c929af6e-4bcb-462e-806a-2be80a324c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression rand search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43670c87-c0d2-4cd9-aed5-5acad7c89587",
   "metadata": {},
   "source": [
    "## Logistic regression using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cab4789-9495-4828-9970-34b79f055510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "The best recall score is 0.8397435897435898\n",
      "... with parameters: {'max_iter': 566, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sscore_measure = \"recall\"\n",
    "kfolds = 5\n",
    "max_iter = rand_search.best_params_['max_iter']\n",
    "penalty = rand_search.best_params_['penalty']\n",
    "solver = rand_search.best_params_['solver']\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter': np.arange(max_iter-10,max_iter+10),  \n",
    "    'penalty': [penalty],\n",
    "    'solver': [solver]\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = log_reg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallLogistic = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6287280-b669-4aee-a337-e351f2c18213",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f8122-5143-4078-b068-0ff0ede6743f",
   "metadata": {},
   "source": [
    "## SVM classification model using Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a90f1eb-3b9d-4ab7-b38b-304a888dc58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 174 is smaller than n_iter=500. Running 174 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 174 candidates, totalling 870 fits\n",
      "The best recall score is 0.9371794871794872\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(1,30),   \n",
    "    'gamma': ['scale','auto'],\n",
    "    'kernel':['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05a52854-0c9f-45c4-843c-1cdad641ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590a014-aac9-4489-9377-80294b745553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2411e8-1a13-443c-b931-5365eacccdc9",
   "metadata": {},
   "source": [
    "## SVM classification model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be205afa-2666-4694-8004-12b4d00c72df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best recall score is 0.9371794871794872\n",
      "... with parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 255, in fit\n",
      "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 315, in _dense_fit\n",
      "    ) = libsvm.fit(\n",
      "  File \"sklearn\\svm\\_libsvm.pyx\", line 189, in sklearn.svm._libsvm.fit\n",
      "ValueError: C <= 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.93717949 0.92179487]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [nan nan  1.  1.]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallSVM = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65e773ee-db8e-4c53-b75b-520fa382d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a1c0c-8a2c-4bb5-93be-d132e59da060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc48e5bf-2723-4a65-a265-7e4dfdf233b7",
   "metadata": {},
   "source": [
    "## Decision tree using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcd363aa-6a2a-4874-99fe-964c6eac24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'min_samples_split': 80, 'min_samples_leaf': 36, 'min_impurity_decrease': 0.0011, 'max_leaf_nodes': 74, 'max_depth': 44, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [1.         1.         1.         1.         1.         0.85384615\n",
      " 0.77435897 0.85512821 0.66153846 0.82564103 0.71153846 1.\n",
      " 1.         1.         1.         0.88717949 0.95384615 0.82820513\n",
      " 1.         0.95384615 0.82564103 1.         0.95384615 1.\n",
      " 0.82564103 1.         1.         1.         1.         0.88717949\n",
      " 1.         1.         0.82564103 1.         1.         1.\n",
      " 0.82564103 0.85512821 1.         0.77820513 1.         1.\n",
      " 0.82564103 1.         0.77820513 0.88717949 0.80897436 0.82564103\n",
      " 1.         0.82564103 1.         1.         1.         0.77820513\n",
      " 1.         1.         1.         1.         0.82564103 1.\n",
      " 0.82564103 1.         1.         1.         0.95384615 1.\n",
      " 0.71153846 1.         1.         1.         1.         1.\n",
      " 1.         0.69487179 1.         1.         1.         1.\n",
      " 1.         1.         0.82564103 1.         1.         1.\n",
      " 1.         0.71153846 1.         0.66153846 0.71153846 1.\n",
      " 1.         1.         0.82564103 1.         0.82564103 1.\n",
      " 1.         1.         0.88717949 1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.71153846 0.79487179 1.         1.         1.         1.\n",
      " 0.85512821 0.82564103 0.82564103 0.66153846 1.         1.\n",
      " 1.         0.82564103 1.         1.         1.         0.82564103\n",
      " 1.         0.77820513 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.82564103 1.         1.         0.82564103 1.\n",
      " 1.         0.88717949 1.         1.         1.         1.\n",
      " 0.66153846 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 0.82564103 1.         0.82564103 1.         1.         1.\n",
      " 0.74487179 1.         1.         0.82564103 1.         1.\n",
      " 0.88717949 0.75897436 1.         0.82564103 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.82564103 1.         1.         0.79487179\n",
      " 1.         1.         1.         1.         1.         0.74487179\n",
      " 1.         1.         1.         0.90384615 0.80384615 1.\n",
      " 0.82564103 1.         0.66153846 0.75641026 0.82564103 1.\n",
      "        nan 1.         1.         0.88717949 0.80897436 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.80384615 1.         0.69358974 1.\n",
      " 1.         1.         1.         0.82564103 1.         1.\n",
      " 0.71153846 0.88717949 1.         1.         0.78974359 1.\n",
      " 0.88717949 1.         0.66153846 1.         1.         0.82564103\n",
      " 1.         1.         1.         1.         0.85641026 0.66153846\n",
      " 1.         0.71153846 1.         1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.80384615 1.         0.69358974 1.         1.\n",
      " 1.         0.82564103 1.         0.77820513        nan 0.85641026\n",
      " 1.         1.         1.         1.         0.88717949 0.88717949\n",
      " 1.         1.         0.71153846 1.         1.         1.\n",
      " 0.82564103 0.74487179 1.         1.         1.         1.\n",
      " 1.         0.71153846 0.80384615 0.82564103 1.         0.77435897\n",
      " 1.         0.88717949 1.         1.         0.82564103        nan\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.80384615\n",
      " 1.         0.95384615 1.         0.71153846 1.         1.\n",
      " 1.         1.         0.80897436 1.         1.         0.82564103\n",
      " 0.77820513 1.         1.         1.         1.         1.\n",
      " 1.         0.71153846 1.         0.82820513 1.         1.\n",
      " 0.82564103 0.88717949 1.         1.         1.         0.88717949\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.80384615 0.80384615 1.         1.\n",
      " 1.         0.88717949 1.                nan 1.         0.82564103\n",
      " 0.72564103 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 1.         1.         1.         1.         1.         0.71153846\n",
      " 1.         0.78974359 1.         0.67820513 0.82564103 1.\n",
      " 0.67820513 1.         0.69487179 1.         1.         0.88717949\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.82564103 1.         0.74487179\n",
      " 1.         0.82564103 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.77820513 0.82820513\n",
      " 1.         1.         1.         1.         0.82564103 1.\n",
      " 1.         1.         1.         0.71153846 0.95384615 0.80384615\n",
      " 1.         1.         0.88717949 1.         0.82564103 1.\n",
      " 1.         1.                nan 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.77820513\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 1.         0.88717949 1.         0.77820513 1.         1.\n",
      " 0.80897436 0.72564103 1.         1.         0.82564103 0.82564103\n",
      " 1.         1.        ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         1.         1.         1.         0.88334694\n",
      " 0.79861224 0.82293878 0.72995918 0.82277551 0.73330612 1.\n",
      " 1.         1.         1.         0.85853061 0.92653061 0.78530612\n",
      " 1.         0.92653061 0.82595918 1.         0.92653061 1.\n",
      " 0.82277551 1.         1.         1.         1.         0.81053061\n",
      " 1.         1.         0.82595918 1.         1.         1.\n",
      " 0.82277551 0.83518367 1.         0.75730612 1.         1.\n",
      " 0.79395918 1.         0.75730612 0.81053061 0.77395918 0.82277551\n",
      " 1.         0.82277551 1.         1.         1.         0.75730612\n",
      " 1.         1.         1.         1.         0.82595918 1.\n",
      " 0.82277551 1.         1.         1.         0.92653061 1.\n",
      " 0.73330612 1.         1.         1.         1.         1.\n",
      " 1.         0.74595918 1.         1.         1.         1.\n",
      " 1.         1.         0.82277551 1.         1.         1.\n",
      " 1.         0.73330612 1.         0.72995918 0.73330612 1.\n",
      " 1.         1.         0.82277551 1.         0.79395918 1.\n",
      " 1.         1.         0.81053061 1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.73330612 0.78195918 1.         1.         1.         1.\n",
      " 0.82293878 0.82595918 0.82277551 0.72995918 1.         1.\n",
      " 1.         0.82277551 1.         1.         1.         0.79395918\n",
      " 1.         0.75730612 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.82595918 1.         1.         0.82277551 1.\n",
      " 1.         0.85853061 1.         1.         1.         1.\n",
      " 0.72995918 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82277551\n",
      " 0.82277551 1.         0.82277551 1.         1.         1.\n",
      " 0.74595918 1.         1.         0.79395918 1.         1.\n",
      " 0.85853061 0.77820408 1.         0.82277551 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79395918 1.         1.         0.78195918\n",
      " 1.         1.         1.         1.         1.         0.74595918\n",
      " 1.         1.         1.         0.89126531 0.79453061 1.\n",
      " 0.82277551 1.         0.72995918 0.85493878 0.82277551 1.\n",
      "        nan 1.         1.         0.85853061 0.76171429 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79453061 1.         0.83036735 1.\n",
      " 1.         1.         1.         0.82277551 1.         1.\n",
      " 0.73330612 0.85853061 1.         1.         0.83461224 1.\n",
      " 0.85853061 1.         0.76195918 1.         1.         0.82277551\n",
      " 1.         1.         1.         1.         0.84702041 0.75395918\n",
      " 1.         0.73330612 1.         1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.79453061 1.         0.83036735 1.         1.\n",
      " 1.         0.79395918 1.         0.75730612        nan 0.85110204\n",
      " 1.         1.         1.         1.         0.81053061 0.85853061\n",
      " 1.         1.         0.73330612 1.         1.         1.\n",
      " 0.82595918 0.74595918 1.         1.         1.         1.\n",
      " 1.         0.73330612 0.79453061 0.82277551 1.         0.79861224\n",
      " 1.         0.85853061 1.         1.         0.82277551        nan\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.79453061\n",
      " 1.         0.92653061 1.         0.73330612 1.         1.\n",
      " 1.         1.         0.77395918 1.         1.         0.79395918\n",
      " 0.75730612 1.         1.         1.         1.         1.\n",
      " 1.         0.73330612 1.         0.78530612 1.         1.\n",
      " 0.82277551 0.81053061 1.         1.         1.         0.85853061\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79453061 0.79453061 1.         1.\n",
      " 1.         0.85853061 1.                nan 1.         0.82277551\n",
      " 0.85893878 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82595918\n",
      " 1.         1.         1.         1.         1.         0.73330612\n",
      " 1.         0.82310204 1.         0.80995918 0.82277551 1.\n",
      " 0.80995918 1.         0.74595918 1.         1.         0.85853061\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.82595918 1.         0.74595918\n",
      " 1.         0.79395918 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.76195918 0.78530612\n",
      " 1.         1.         1.         1.         0.79395918 1.\n",
      " 1.         1.         1.         0.73330612 0.92653061 0.79453061\n",
      " 1.         1.         0.85853061 1.         0.82277551 1.\n",
      " 1.         1.                nan 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.75730612\n",
      " 1.         1.         1.         1.         1.         0.82277551\n",
      " 1.         0.85853061 1.         0.75730612 1.         1.\n",
      " 0.77395918 0.83077551 1.         1.         0.82595918 0.82277551\n",
      " 1.         1.        ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {random_search.best_score_}\")\n",
    "print(f\"... with parameters: {random_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cf1acb1-e429-4cbc-87c2-5c3b07472bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, random_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree_random\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce7e17-9f49-4a0f-ac84-11b919e029d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137162bf-ec65-494b-926b-e22250b0a388",
   "metadata": {},
   "source": [
    "## Decision tree using Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37e00c4e-977a-485b-ac34-ea3048097571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best recall score is 0.8551282051282051\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 6, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6b0e0b2-c4ac-4f8c-a2fa-b6ebb5d4a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree_grid\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2550a-bfab-49ad-9bef-93b9f2015095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d92dd4-98a3-4b37-983a-03d7251079de",
   "metadata": {},
   "source": [
    "## 5.0 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424eab6-ef4e-4c6b-a868-83d8a14b075b",
   "metadata": {},
   "source": [
    "Sorted by accuracy, the best models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1073b757-ae69-4369-91ca-84fce5415562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree_random</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree_grid</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression rand search</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression grid search</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Random search</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM grid search</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  Accuracy  Precision    Recall        F1\n",
       "0                     Dtree_random     0.550   0.681818  0.576923  0.625000\n",
       "0                       Dtree_grid     0.650   0.800000  0.615385  0.695652\n",
       "0  Logistic Regression rand search     0.650   0.700000  0.807692  0.750000\n",
       "0  Logistic Regression grid search     0.650   0.700000  0.807692  0.750000\n",
       "0                SVM Random search     0.675   0.696970  0.884615  0.779661\n",
       "0                  SVM grid search     0.675   0.696970  0.884615  0.779661"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff4078-859f-4ca0-a7c6-e914a51a666d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b73ae-b461-4725-b6e3-412aae67215e",
   "metadata": {},
   "source": [
    "The dataset is connected to predicting the purchase of the Apple M1 Mac book. The purpose of this data set is to predict whether or not the customer will buy the M1 Macbook. As a result, the goal of this assignment is to establish which features you will employ to analyze purchase behaviors and how these features impact the sales of the apple mac book.\n",
    "\n",
    "I chose the recall performance indicator for this dataset because recall is used to quantify the proportion of true positives out of all possible outcomes.\n",
    "\n",
    "According to the above results,  it is observed that SVM performs well out of all the three models with 88.46 \n",
    "percentage accuarcy followed by logistic regression with random and grid search with 80.67 percentage accuracy and decision tree with\n",
    "grid search with 61.53 percentage accuracy.\n",
    "Hence the SVM model outperforms with the remaining model when recall is considered as the performance\n",
    "metric. \n",
    "\n",
    "Using recall, the decision tree using grid search model is observed to be the least performing model.\n",
    "However, when precision is considered as the performance metric, the decision tree model with grid search is considered to be the best model, followed by logistic regression using random search and grid search, and SVM is considered.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e4dc8-2906-4aac-b92c-3e228b8f73ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
