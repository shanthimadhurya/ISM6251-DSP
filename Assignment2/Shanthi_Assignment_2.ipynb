{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d496d733-0cdc-4d33-8fdb-b98125637c1d",
   "metadata": {},
   "source": [
    "## Data modeling using generated data file after performing data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a099445-d335-4d07-aaee-8047b210ec17",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17715a3c-094e-4c4a-a67d-0a808312a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as t2\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4774cc6-cf24-4dc0-b166-82bb6cdb53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn import datasets\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39289a7-bf73-42a5-9bf7-a3b1bce8a30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a099a185-81cf-49da-9a62-d27893a1b767",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55a67b27-f7cf-433e-99b4-b089cb22799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"appledata_train_X.csv\")\n",
    "X_test = pd.read_csv(\"appledata_test_X.csv\")\n",
    "y_train = pd.read_csv(\"appledata_train_y.csv\")\n",
    "y_test = pd.read_csv(\"appledata_test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5f6b8-b67b-4dab-80b0-a9580e385bb2",
   "metadata": {},
   "source": [
    "## 3. creating performance data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a43e12b3-0270-4c0f-9b93-1a8c286588b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c7611-a45a-4e04-9d1a-3fa0eb1d2620",
   "metadata": {},
   "source": [
    "## 4. Logistic regression using Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4080a4c-b44d-4b79-ad11-615eed057270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.8397435897435898\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1255 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "66 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "91 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "82 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "66 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet', 'none' (deprecated)} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "270 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "116 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'none' (deprecated), 'l2', 'l1', 'elasticnet'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "68 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'none' (deprecated), 'elasticnet', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.83974359        nan        nan        nan\n",
      " 0.83974359 0.82435897 0.82435897        nan        nan        nan\n",
      " 0.83974359        nan 0.82435897 0.83974359 0.82435897        nan\n",
      "        nan        nan        nan 0.83974359 0.82435897 0.82435897\n",
      " 0.82435897        nan        nan        nan 0.82435897        nan\n",
      " 0.82435897 0.82435897 0.82435897        nan        nan        nan\n",
      "        nan        nan 0.83974359 0.82435897 0.82435897        nan\n",
      " 0.82435897        nan        nan        nan        nan        nan\n",
      " 0.83974359 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897        nan 0.82435897        nan        nan 0.82435897\n",
      " 0.82435897 0.83974359 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.83974359 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897 0.82435897        nan        nan\n",
      " 0.83974359 0.83974359        nan 0.83974359 0.82435897 0.83974359\n",
      " 0.82435897 0.82435897 0.83974359 0.82435897 0.82435897 0.83974359\n",
      " 0.82435897        nan 0.83974359 0.83974359        nan        nan\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897 0.82435897        nan        nan 0.82435897 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.82435897        nan        nan\n",
      "        nan 0.82435897 0.82435897        nan        nan 0.82435897\n",
      " 0.82435897 0.83974359        nan 0.82435897 0.82435897        nan\n",
      "        nan 0.82435897 0.82435897        nan        nan 0.83974359\n",
      " 0.82435897        nan        nan        nan 0.83974359        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82435897        nan 0.82435897        nan 0.82435897        nan\n",
      "        nan 0.82435897        nan        nan 0.82435897        nan\n",
      " 0.83974359 0.83974359        nan        nan        nan 0.82435897\n",
      "        nan        nan 0.82435897        nan        nan 0.83974359\n",
      " 0.82435897 0.83974359        nan 0.82435897 0.82435897 0.82435897\n",
      "        nan 0.82435897        nan 0.82435897        nan        nan\n",
      "        nan 0.82435897        nan        nan 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82435897 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897 0.82435897        nan 0.82435897 0.82435897\n",
      " 0.83974359 0.82435897 0.82435897        nan 0.83974359        nan\n",
      "        nan        nan 0.82435897 0.83974359 0.82435897 0.82435897\n",
      " 0.82435897        nan        nan 0.82435897        nan        nan\n",
      "        nan 0.82435897        nan        nan        nan 0.82435897\n",
      "        nan 0.82435897 0.82435897        nan 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan 0.82435897 0.82435897\n",
      " 0.82435897 0.82435897        nan 0.82435897 0.82435897 0.82435897\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      " 0.82435897 0.83974359 0.82435897        nan        nan        nan\n",
      "        nan 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897        nan        nan        nan        nan 0.82435897\n",
      "        nan 0.82435897        nan        nan 0.82435897        nan\n",
      " 0.82435897        nan 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897 0.82435897        nan 0.82435897 0.83974359\n",
      " 0.82435897        nan 0.83974359 0.82435897        nan 0.82435897\n",
      "        nan        nan 0.82435897 0.82435897        nan        nan\n",
      " 0.82435897 0.82435897        nan 0.83974359        nan        nan\n",
      "        nan        nan 0.82435897 0.82435897        nan        nan\n",
      "        nan 0.83974359        nan        nan 0.82435897 0.82435897\n",
      "        nan        nan 0.82435897        nan 0.83974359        nan\n",
      " 0.83974359        nan        nan 0.83974359 0.83974359 0.82435897\n",
      "        nan        nan        nan 0.82435897 0.82435897        nan\n",
      "        nan 0.82435897 0.83974359        nan 0.82435897        nan\n",
      "        nan        nan        nan 0.82435897        nan 0.83974359\n",
      "        nan 0.82435897        nan        nan        nan        nan\n",
      " 0.82435897        nan        nan 0.83974359        nan 0.82435897\n",
      " 0.82435897        nan        nan        nan 0.83974359 0.83974359\n",
      " 0.83974359 0.82435897        nan        nan        nan        nan\n",
      " 0.83974359 0.82435897        nan        nan 0.82435897 0.82435897\n",
      "        nan 0.82435897        nan        nan 0.82435897 0.83974359\n",
      "        nan        nan        nan 0.82435897        nan 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.83974359        nan 0.82435897\n",
      "        nan 0.82435897        nan 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897 0.83974359 0.83974359        nan 0.82435897\n",
      " 0.83974359        nan        nan        nan 0.82435897 0.82435897\n",
      "        nan 0.83974359 0.82435897 0.82435897 0.83974359 0.82435897\n",
      " 0.82435897        nan 0.82435897 0.83974359 0.82435897 0.82435897\n",
      "        nan        nan        nan        nan 0.83974359        nan\n",
      "        nan 0.82435897 0.82435897 0.82435897        nan 0.82435897\n",
      "        nan 0.82435897        nan        nan        nan 0.82435897\n",
      " 0.82435897 0.82435897        nan 0.82435897        nan 0.82435897\n",
      "        nan        nan        nan 0.83974359 0.82435897 0.82435897\n",
      "        nan        nan        nan 0.83974359        nan 0.82435897\n",
      "        nan        nan 0.82435897 0.83974359 0.82435897 0.83974359\n",
      " 0.82435897        nan        nan        nan        nan 0.82435897\n",
      " 0.83974359 0.83974359 0.83974359        nan        nan        nan\n",
      "        nan        nan 0.82435897        nan        nan        nan\n",
      " 0.82435897        nan        nan 0.83974359 0.83974359        nan\n",
      " 0.83974359 0.82435897]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.90734694        nan        nan        nan\n",
      " 0.90734694 0.90326531 0.91134694        nan        nan        nan\n",
      " 0.90734694        nan 0.90326531 0.90734694 0.91134694        nan\n",
      "        nan        nan        nan 0.90734694 0.91134694 0.90326531\n",
      " 0.90326531        nan        nan        nan 0.90326531        nan\n",
      " 0.91134694 0.90326531 0.90326531        nan        nan        nan\n",
      "        nan        nan 0.90734694 0.91134694 0.91134694        nan\n",
      " 0.90326531        nan        nan        nan        nan        nan\n",
      " 0.90734694 0.90326531        nan        nan        nan        nan\n",
      " 0.90326531 0.90326531 0.91134694 0.90326531        nan 0.91134694\n",
      " 0.90326531        nan 0.90326531        nan        nan 0.90326531\n",
      " 0.90326531 0.90734694 0.91134694 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694 0.90734694 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694 0.90326531 0.90326531        nan        nan\n",
      " 0.90734694 0.90734694        nan 0.90734694 0.90326531 0.90734694\n",
      " 0.90326531 0.90326531 0.90734694 0.91134694 0.90326531 0.90734694\n",
      " 0.90326531        nan 0.90734694 0.90734694        nan        nan\n",
      "        nan 0.90326531 0.91134694 0.90326531        nan 0.90326531\n",
      " 0.90326531 0.90326531        nan        nan 0.90326531 0.90326531\n",
      " 0.90326531        nan 0.90326531 0.90326531        nan        nan\n",
      "        nan 0.91134694 0.90326531        nan        nan 0.91134694\n",
      " 0.90326531 0.90734694        nan 0.90326531 0.90326531        nan\n",
      "        nan 0.91134694 0.90326531        nan        nan 0.90734694\n",
      " 0.90326531        nan        nan        nan 0.90734694        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.90326531        nan 0.90326531        nan 0.90326531        nan\n",
      "        nan 0.90326531        nan        nan 0.91134694        nan\n",
      " 0.90734694 0.90734694        nan        nan        nan 0.91134694\n",
      "        nan        nan 0.90326531        nan        nan 0.90734694\n",
      " 0.90326531 0.90734694        nan 0.90326531 0.90326531 0.91134694\n",
      "        nan 0.90326531        nan 0.90326531        nan        nan\n",
      "        nan 0.90326531        nan        nan 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.90326531 0.90326531        nan 0.90326531\n",
      "        nan 0.90326531 0.91134694        nan 0.90326531 0.90326531\n",
      " 0.90734694 0.91134694 0.91134694        nan 0.90734694        nan\n",
      "        nan        nan 0.90326531 0.90734694 0.90326531 0.91134694\n",
      " 0.90326531        nan        nan 0.90326531        nan        nan\n",
      "        nan 0.90326531        nan        nan        nan 0.91134694\n",
      "        nan 0.91134694 0.90326531        nan 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan 0.91134694 0.91134694\n",
      " 0.90326531 0.90326531        nan 0.90326531 0.90326531 0.90326531\n",
      "        nan 0.90326531 0.91134694 0.90326531        nan 0.91134694\n",
      " 0.90326531 0.90734694 0.91134694        nan        nan        nan\n",
      "        nan 0.90326531        nan        nan        nan        nan\n",
      " 0.90326531        nan        nan        nan        nan 0.90326531\n",
      "        nan 0.90326531        nan        nan 0.90326531        nan\n",
      " 0.91134694        nan 0.90326531 0.90326531        nan        nan\n",
      " 0.91134694 0.91134694 0.90326531        nan 0.90326531 0.90734694\n",
      " 0.91134694        nan 0.90734694 0.90326531        nan 0.90326531\n",
      "        nan        nan 0.90326531 0.90326531        nan        nan\n",
      " 0.90326531 0.91134694        nan 0.90734694        nan        nan\n",
      "        nan        nan 0.90326531 0.90326531        nan        nan\n",
      "        nan 0.90734694        nan        nan 0.90326531 0.90326531\n",
      "        nan        nan 0.90326531        nan 0.90734694        nan\n",
      " 0.90734694        nan        nan 0.90734694 0.90734694 0.91134694\n",
      "        nan        nan        nan 0.90326531 0.90326531        nan\n",
      "        nan 0.90326531 0.90734694        nan 0.90326531        nan\n",
      "        nan        nan        nan 0.90326531        nan 0.90734694\n",
      "        nan 0.90326531        nan        nan        nan        nan\n",
      " 0.91134694        nan        nan 0.90734694        nan 0.90326531\n",
      " 0.91134694        nan        nan        nan 0.90734694 0.90734694\n",
      " 0.90734694 0.90326531        nan        nan        nan        nan\n",
      " 0.90734694 0.91134694        nan        nan 0.90326531 0.90326531\n",
      "        nan 0.90326531        nan        nan 0.90326531 0.90734694\n",
      "        nan        nan        nan 0.90326531        nan 0.90326531\n",
      " 0.90326531        nan 0.90326531 0.90734694        nan 0.91134694\n",
      "        nan 0.90326531        nan 0.91134694        nan 0.90326531\n",
      "        nan 0.90326531 0.90734694 0.90734694        nan 0.90326531\n",
      " 0.90734694        nan        nan        nan 0.90326531 0.91134694\n",
      "        nan 0.90734694 0.90326531 0.90326531 0.90734694 0.90326531\n",
      " 0.91134694        nan 0.90326531 0.90734694 0.90326531 0.90326531\n",
      "        nan        nan        nan        nan 0.90734694        nan\n",
      "        nan 0.90326531 0.90326531 0.90326531        nan 0.91134694\n",
      "        nan 0.90326531        nan        nan        nan 0.90326531\n",
      " 0.90326531 0.90326531        nan 0.91134694        nan 0.90326531\n",
      "        nan        nan        nan 0.90734694 0.91134694 0.90326531\n",
      "        nan        nan        nan 0.90734694        nan 0.90326531\n",
      "        nan        nan 0.90326531 0.90734694 0.91134694 0.90734694\n",
      " 0.90326531        nan        nan        nan        nan 0.90326531\n",
      " 0.90734694 0.90734694 0.90734694        nan        nan        nan\n",
      "        nan        nan 0.91134694        nan        nan        nan\n",
      " 0.90326531        nan        nan 0.90734694 0.90734694        nan\n",
      " 0.90734694 0.91134694]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter':np.arange(500,1000),\n",
    "    'penalty': ['None','l1','l2','elasticnet'],\n",
    "    'solver':['saga','liblinear']\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = log_reg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c929af6e-4bcb-462e-806a-2be80a324c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression rand search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43670c87-c0d2-4cd9-aed5-5acad7c89587",
   "metadata": {},
   "source": [
    "## Logistic regression using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1cab4789-9495-4828-9970-34b79f055510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "The best recall score is 0.8397435897435898\n",
      "... with parameters: {'max_iter': 566, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sscore_measure = \"recall\"\n",
    "kfolds = 5\n",
    "max_iter = rand_search.best_params_['max_iter']\n",
    "penalty = rand_search.best_params_['penalty']\n",
    "solver = rand_search.best_params_['solver']\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter': np.arange(max_iter-10,max_iter+10),  \n",
    "    'penalty': [penalty],\n",
    "    'solver': [solver]\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = log_reg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallLogistic = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6287280-b669-4aee-a337-e351f2c18213",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f8122-5143-4078-b068-0ff0ede6743f",
   "metadata": {},
   "source": [
    "## SVM classification model using Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a90f1eb-3b9d-4ab7-b38b-304a888dc58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 174 is smaller than n_iter=500. Running 174 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 174 candidates, totalling 870 fits\n",
      "The best recall score is 0.9371794871794872\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(1,30),   \n",
    "    'gamma': ['scale','auto'],\n",
    "    'kernel':['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05a52854-0c9f-45c4-843c-1cdad641ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590a014-aac9-4489-9377-80294b745553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2411e8-1a13-443c-b931-5365eacccdc9",
   "metadata": {},
   "source": [
    "## SVM classification model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be205afa-2666-4694-8004-12b4d00c72df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best recall score is 0.9371794871794872\n",
      "... with parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 180, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of SVC must be a float in the range (0.0, inf). Got -1 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 180, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of SVC must be a float in the range (0.0, inf). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.93717949 0.92179487]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [nan nan  1.  1.]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallSVM = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65e773ee-db8e-4c53-b75b-520fa382d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a1c0c-8a2c-4bb5-93be-d132e59da060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc48e5bf-2723-4a65-a265-7e4dfdf233b7",
   "metadata": {},
   "source": [
    "## Decision tree using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bcd363aa-6a2a-4874-99fe-964c6eac24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'min_samples_split': 80, 'min_samples_leaf': 36, 'min_impurity_decrease': 0.0011, 'max_leaf_nodes': 74, 'max_depth': 44, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [1.         1.         1.         1.         1.         0.85384615\n",
      " 0.77435897 0.85512821 0.66153846 0.82564103 0.71153846 1.\n",
      " 1.         1.         1.         0.88717949 0.95384615 0.82820513\n",
      " 1.         0.95384615 0.82564103 1.         0.95384615 1.\n",
      " 0.82564103 1.         1.         1.         1.         0.88717949\n",
      " 1.         1.         0.82564103 1.         1.         1.\n",
      " 0.82564103 0.85512821 1.         0.77820513 1.         1.\n",
      " 0.82564103 1.         0.77820513 0.88717949 0.80897436 0.82564103\n",
      " 1.         0.82564103 1.         1.         1.         0.77820513\n",
      " 1.         1.         1.         1.         0.82564103 1.\n",
      " 0.82564103 1.         1.         1.         0.95384615 1.\n",
      " 0.71153846 1.         1.         1.         1.         1.\n",
      " 1.         0.69487179 1.         1.         1.         1.\n",
      " 1.         1.         0.82564103 1.         1.         1.\n",
      " 1.         0.71153846 1.         0.66153846 0.71153846 1.\n",
      " 1.         1.         0.82564103 1.         0.82564103 1.\n",
      " 1.         1.         0.88717949 1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.71153846 0.79487179 1.         1.         1.         1.\n",
      " 0.85512821 0.82564103 0.82564103 0.66153846 1.         1.\n",
      " 1.         0.82564103 1.         1.         1.         0.82564103\n",
      " 1.         0.77820513 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.82564103 1.         1.         0.82564103 1.\n",
      " 1.         0.88717949 1.         1.         1.         1.\n",
      " 0.66153846 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 0.82564103 1.         0.82564103 1.         1.         1.\n",
      " 0.74487179 1.         1.         0.82564103 1.         1.\n",
      " 0.88717949 0.75897436 1.         0.82564103 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.82564103 1.         1.         0.79487179\n",
      " 1.         1.         1.         1.         1.         0.74487179\n",
      " 1.         1.         1.         0.90384615 0.80384615 1.\n",
      " 0.82564103 1.         0.66153846 0.72564103 0.82564103 1.\n",
      "        nan 1.         1.         0.88717949 0.80897436 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.80384615 1.         0.69358974 1.\n",
      " 1.         1.         1.         0.82564103 1.         1.\n",
      " 0.71153846 0.88717949 1.         1.         0.80641026 1.\n",
      " 0.88717949 1.         0.66153846 1.         1.         0.82564103\n",
      " 1.         1.         1.         1.         0.85641026 0.66153846\n",
      " 1.         0.71153846 1.         1.         0.95384615 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.80384615 1.         0.69358974 1.         1.\n",
      " 1.         0.82564103 1.         0.77820513        nan 0.85641026\n",
      " 1.         1.         1.         1.         0.88717949 0.88717949\n",
      " 1.         1.         0.71153846 1.         1.         1.\n",
      " 0.82564103 0.74487179 1.         1.         1.         1.\n",
      " 1.         0.71153846 0.80384615 0.82564103 1.         0.77435897\n",
      " 1.         0.88717949 1.         1.         0.82564103        nan\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.80384615\n",
      " 1.         0.95384615 1.         0.71153846 1.         1.\n",
      " 1.         1.         0.80897436 1.         1.         0.82564103\n",
      " 0.77820513 1.         1.         1.         1.         1.\n",
      " 1.         0.71153846 1.         0.82820513 1.         1.\n",
      " 0.82564103 0.88717949 1.         1.         1.         0.88717949\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.80384615 0.80384615 1.         1.\n",
      " 1.         0.88717949 1.                nan 1.         0.82564103\n",
      " 0.75641026 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 1.         1.         1.         1.         1.         0.71153846\n",
      " 1.         0.78974359 1.         0.67820513 0.82564103 1.\n",
      " 0.67820513 1.         0.69487179 1.         1.         0.88717949\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.82564103 1.         0.74487179\n",
      " 1.         0.82564103 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.77820513 0.82820513\n",
      " 1.         1.         1.         1.         0.82564103 1.\n",
      " 1.         1.         1.         0.71153846 0.95384615 0.80384615\n",
      " 1.         1.         0.88717949 1.         0.82564103 1.\n",
      " 1.         1.                nan 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.77820513\n",
      " 1.         1.         1.         1.         1.         0.82564103\n",
      " 1.         0.88717949 1.         0.77820513 1.         1.\n",
      " 0.80897436 0.72564103 1.         1.         0.82564103 0.82564103\n",
      " 1.         1.        ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [1.         1.         1.         1.         1.         0.88334694\n",
      " 0.79861224 0.82293878 0.72995918 0.82277551 0.73330612 1.\n",
      " 1.         1.         1.         0.85853061 0.92653061 0.78530612\n",
      " 1.         0.92653061 0.82595918 1.         0.92653061 1.\n",
      " 0.82277551 1.         1.         1.         1.         0.81053061\n",
      " 1.         1.         0.82595918 1.         1.         1.\n",
      " 0.82277551 0.83518367 1.         0.75730612 1.         1.\n",
      " 0.79395918 1.         0.75730612 0.81053061 0.77395918 0.82277551\n",
      " 1.         0.82277551 1.         1.         1.         0.75730612\n",
      " 1.         1.         1.         1.         0.82595918 1.\n",
      " 0.82277551 1.         1.         1.         0.92653061 1.\n",
      " 0.73330612 1.         1.         1.         1.         1.\n",
      " 1.         0.74595918 1.         1.         1.         1.\n",
      " 1.         1.         0.82277551 1.         1.         1.\n",
      " 1.         0.73330612 1.         0.72995918 0.73330612 1.\n",
      " 1.         1.         0.82277551 1.         0.79395918 1.\n",
      " 1.         1.         0.81053061 1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.73330612 0.78195918 1.         1.         1.         1.\n",
      " 0.82293878 0.82595918 0.82277551 0.72995918 1.         1.\n",
      " 1.         0.82277551 1.         1.         1.         0.79395918\n",
      " 1.         0.75730612 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.82595918 1.         1.         0.82277551 1.\n",
      " 1.         0.85853061 1.         1.         1.         1.\n",
      " 0.72995918 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82277551\n",
      " 0.82277551 1.         0.82277551 1.         1.         1.\n",
      " 0.74595918 1.         1.         0.79395918 1.         1.\n",
      " 0.85853061 0.77820408 1.         0.82277551 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79395918 1.         1.         0.78195918\n",
      " 1.         1.         1.         1.         1.         0.74595918\n",
      " 1.         1.         1.         0.89126531 0.79453061 1.\n",
      " 0.82277551 1.         0.72995918 0.85493878 0.82277551 1.\n",
      "        nan 1.         1.         0.85853061 0.76171429 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79453061 1.         0.83036735 1.\n",
      " 1.         1.         1.         0.82277551 1.         1.\n",
      " 0.73330612 0.85853061 1.         1.         0.83461224 1.\n",
      " 0.85853061 1.         0.76195918 1.         1.         0.82277551\n",
      " 1.         1.         1.         1.         0.84702041 0.75395918\n",
      " 1.         0.73330612 1.         1.         0.92653061 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.79453061 1.         0.83036735 1.         1.\n",
      " 1.         0.79395918 1.         0.75730612        nan 0.85110204\n",
      " 1.         1.         1.         1.         0.81053061 0.85853061\n",
      " 1.         1.         0.73330612 1.         1.         1.\n",
      " 0.82595918 0.74595918 1.         1.         1.         1.\n",
      " 1.         0.73330612 0.79453061 0.82277551 1.         0.79861224\n",
      " 1.         0.85853061 1.         1.         0.82277551        nan\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.79453061\n",
      " 1.         0.92653061 1.         0.73330612 1.         1.\n",
      " 1.         1.         0.77395918 1.         1.         0.79395918\n",
      " 0.75730612 1.         1.         1.         1.         1.\n",
      " 1.         0.73330612 1.         0.78530612 1.         1.\n",
      " 0.82277551 0.81053061 1.         1.         1.         0.85853061\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.79453061 0.79453061 1.         1.\n",
      " 1.         0.85853061 1.                nan 1.         0.82277551\n",
      " 0.85893878 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.82595918\n",
      " 1.         1.         1.         1.         1.         0.73330612\n",
      " 1.         0.82310204 1.         0.80995918 0.82277551 1.\n",
      " 0.80995918 1.         0.74595918 1.         1.         0.85853061\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.82595918 1.         0.74595918\n",
      " 1.         0.79395918 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.76195918 0.78530612\n",
      " 1.         1.         1.         1.         0.79395918 1.\n",
      " 1.         1.         1.         0.73330612 0.92653061 0.79453061\n",
      " 1.         1.         0.85853061 1.         0.82277551 1.\n",
      " 1.         1.                nan 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.75730612\n",
      " 1.         1.         1.         1.         1.         0.82277551\n",
      " 1.         0.85853061 1.         0.75730612 1.         1.\n",
      " 0.77395918 0.83893878 1.         1.         0.82595918 0.82277551\n",
      " 1.         1.        ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {random_search.best_score_}\")\n",
    "print(f\"... with parameters: {random_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9cf1acb1-e429-4cbc-87c2-5c3b07472bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, random_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree_random\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce7e17-9f49-4a0f-ac84-11b919e029d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137162bf-ec65-494b-926b-e22250b0a388",
   "metadata": {},
   "source": [
    "## Decision tree using Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37e00c4e-977a-485b-ac34-ea3048097571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best recall score is 0.8551282051282051\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 6, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6b0e0b2-c4ac-4f8c-a2fa-b6ebb5d4a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree_grid\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2550a-bfab-49ad-9bef-93b9f2015095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbe612b8-4ee3-4388-b13e-74e8f049360e",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ceca0a56-d2c7-4819-af83-9b7d21007cf5",
   "metadata": {
    "id": "5WfGTWb3hYd-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.48 s\n",
      "Wall time: 542 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a1d8ece6-5de5-42db-a694-d7541f18b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 5.76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956af14b-f87f-46a8-87a3-e5cb7b5ae382",
   "metadata": {},
   "source": [
    "## NN With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24befa10-8415-461c-92cb-056eb96bc550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 6, 'min_samples_split': 30}\n",
      "CPU times: total: 1.45 s\n",
      "Wall time: 38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b14432a6-95b9-4669-8876-20f23744c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.65      1.00      0.79        26\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.33      0.50      0.39        40\n",
      "weighted avg       0.42      0.65      0.51        40\n",
      "\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 35.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test=X_test[:len(y_test)]\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"NN_Rand\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fb568-1376-48d1-9cdf-ac20491312e4",
   "metadata": {},
   "source": [
    "## NN With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9525dc17-8af2-4fed-9a38-1d0917663d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "{'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (30,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.005, 'max_iter': 5000, 'solver': 'adam'}\n",
      "CPU times: total: 2.08 s\n",
      "Wall time: 34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanthi\\anaconda3\\envs\\t2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9603632b-58ef-4378-b5bd-bbe7ef58d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46        14\n",
      "           1       0.71      0.77      0.74        26\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.61      0.60      0.60        40\n",
      "weighted avg       0.64      0.65      0.64        40\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 54.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test=X_test[:len(y_test)]\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"NN_GRID\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ab291-4eaf-45f0-9563-632877867503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10e915a-27d5-4f81-9fc6-b140a1770dd2",
   "metadata": {},
   "source": [
    "# Using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb25cb3-90a7-4f94-943e-2487163b1121",
   "metadata": {},
   "source": [
    "## Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4048208b-3d3d-4ae5-892b-5105f6c4ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as t2\n",
    "from tensorflow import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "t2.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebaad2ec-0e59-4160-aaa4-2dfcf92eab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create model stucture\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(10))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) # final layer, 1 categories\n",
    "\n",
    "\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# if you want to overide the defaults for the optimizer....\n",
    "#adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "139eb850-ad65-461e-8953-df511fbc442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6841 - accuracy: 0.5806 - val_loss: 0.6838 - val_accuracy: 0.6750\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6747 - accuracy: 0.6452 - val_loss: 0.6782 - val_accuracy: 0.6750\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6656 - accuracy: 0.6559 - val_loss: 0.6729 - val_accuracy: 0.6500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6569 - accuracy: 0.6667 - val_loss: 0.6677 - val_accuracy: 0.6500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6487 - accuracy: 0.6667 - val_loss: 0.6626 - val_accuracy: 0.6500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6406 - accuracy: 0.6774 - val_loss: 0.6578 - val_accuracy: 0.6500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6328 - accuracy: 0.6774 - val_loss: 0.6532 - val_accuracy: 0.6750\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6250 - accuracy: 0.6882 - val_loss: 0.6488 - val_accuracy: 0.6750\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6174 - accuracy: 0.6882 - val_loss: 0.6447 - val_accuracy: 0.6750\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6099 - accuracy: 0.6882 - val_loss: 0.6409 - val_accuracy: 0.6750\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6025 - accuracy: 0.6882 - val_loss: 0.6374 - val_accuracy: 0.6750\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5953 - accuracy: 0.6882 - val_loss: 0.6341 - val_accuracy: 0.6750\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5880 - accuracy: 0.6882 - val_loss: 0.6311 - val_accuracy: 0.7000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5805 - accuracy: 0.6989 - val_loss: 0.6284 - val_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5730 - accuracy: 0.7312 - val_loss: 0.6257 - val_accuracy: 0.7000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5655 - accuracy: 0.7312 - val_loss: 0.6233 - val_accuracy: 0.6750\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5580 - accuracy: 0.7419 - val_loss: 0.6209 - val_accuracy: 0.6750\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5504 - accuracy: 0.7419 - val_loss: 0.6189 - val_accuracy: 0.6750\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5427 - accuracy: 0.7419 - val_loss: 0.6169 - val_accuracy: 0.6750\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5351 - accuracy: 0.7419 - val_loss: 0.6152 - val_accuracy: 0.7000\n",
      "CPU times: total: 9.02 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the model\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test[:len(y_test)], y_test), \n",
    "                    epochs=20, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2565e07-f967-4c5d-bf7c-61530f32cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6151880025863647, 0.699999988079071]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(X_test[:len(y_test)],y_test, verbose=0)\n",
    "scores\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ed2b401-91b6-4715-b49b-1e4b237469a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.62\n",
      "accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "# let's format this into a better output...\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd5fe2-f7b9-45b1-81c9-ccedeab81b82",
   "metadata": {},
   "source": [
    "## Wide and Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "502854be-53e3-49b3-b9b9-07d23cb053d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=10))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7107df23-c69a-4d5c-a809-f2cc62bde32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20e23c25-925a-486f-a3ab-df65600f2a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7109 - accuracy: 0.4086 - val_loss: 0.6490 - val_accuracy: 0.6500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5756 - accuracy: 0.6667 - val_loss: 0.6087 - val_accuracy: 0.7250\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4613 - accuracy: 0.7742 - val_loss: 0.6172 - val_accuracy: 0.6500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3810 - accuracy: 0.8387 - val_loss: 0.6774 - val_accuracy: 0.6500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3304 - accuracy: 0.8602 - val_loss: 0.7484 - val_accuracy: 0.7000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2847 - accuracy: 0.8925 - val_loss: 0.8238 - val_accuracy: 0.7000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2477 - accuracy: 0.8925 - val_loss: 0.8799 - val_accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2086 - accuracy: 0.9032 - val_loss: 0.9593 - val_accuracy: 0.6500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1782 - accuracy: 0.9247 - val_loss: 1.0818 - val_accuracy: 0.6500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1470 - accuracy: 0.9355 - val_loss: 1.2291 - val_accuracy: 0.6500\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1206 - accuracy: 0.9677 - val_loss: 1.3705 - val_accuracy: 0.6500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0938 - accuracy: 0.9677 - val_loss: 1.5360 - val_accuracy: 0.6500\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0742 - accuracy: 0.9785 - val_loss: 1.7538 - val_accuracy: 0.6000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0598 - accuracy: 0.9785 - val_loss: 2.0109 - val_accuracy: 0.6000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0514 - accuracy: 0.9785 - val_loss: 2.2824 - val_accuracy: 0.6000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0424 - accuracy: 0.9785 - val_loss: 2.5767 - val_accuracy: 0.6500\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0384 - accuracy: 0.9785 - val_loss: 2.8885 - val_accuracy: 0.6500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0348 - accuracy: 0.9785 - val_loss: 3.1966 - val_accuracy: 0.6000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0335 - accuracy: 0.9785 - val_loss: 3.5171 - val_accuracy: 0.6500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0365 - accuracy: 0.9785 - val_loss: 3.7739 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "X_test=X_test[:len(y_test)]\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0495a7ae-3df9-4f5f-a3d1-fb0c31325b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.773873805999756, 0.6000000238418579]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "X_test=X_test[:len(y_test)]\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd9084c7-19f9-4a55-b6d4-a6fa30e2d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.77\n",
      "accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707b87a-7546-4461-a76c-7259348aacd7",
   "metadata": {},
   "source": [
    "## RandomGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6f2803d-cd1a-4e79-b122-171b58b07446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you don't have the following installed, from command line '!pip install scikeras'\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import GlorotNormal\n",
    "\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = t2.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=10)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= t2.keras.initializers.GlorotNormal(), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(t2.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    ann.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6a424-045c-4dc5-8013-43d18bcfe77f",
   "metadata": {},
   "source": [
    "For more information on dense layers and initializers, see the following:\n",
    "* https://keras.io/api/layers/core_layers/dense/\n",
    "* https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab549c81-ef7b-4398-91c0-9d3b6db601ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=40,\n",
    "    dropout = 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92ce4e74-9afc-433a-adff-c69735b64d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'hidden_layer_sizes', 'dropout', 'class_weight'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'model__hidden_layer_sizes': [(70,),(90, ), (100,), (100, 90)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[20, 60, 100],\n",
    "    'epochs':[10, 50, 100],\n",
    "    'optimizer':[\"adam\",'sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b759448-a334-4300-997f-9f247374355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(estimator=keras_clf, param_distributions=params, scoring='accuracy', n_iter=50, cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = rnd_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "406a98a8-a110-4d3e-9e29-438319dbacfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer__learning_rate': 0.0005,\n",
       " 'optimizer': 'adam',\n",
       " 'model__hidden_layer_sizes': (100,),\n",
       " 'model__dropout': 0,\n",
       " 'epochs': 50,\n",
       " 'batch_size': 60}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "129a8fa8-a2fe-432f-97c3-cc95b3351bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer__learning_rate': 0.0005, 'optimizer': 'adam', 'model__hidden_layer_sizes': (100,), 'model__dropout': 0, 'epochs': 50, 'batch_size': 60}\n"
     ]
    }
   ],
   "source": [
    "best_net = rnd_search_cv.best_estimator_\n",
    "print(rnd_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4568d674-391a-4db3-8d4f-021cda31f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.43      0.40        14\n",
      "           1       0.67      0.62      0.64        26\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.52      0.52      0.52        40\n",
      "weighted avg       0.56      0.55      0.56        40\n",
      "\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 386 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test=X_test[:len(y_test)]\n",
    "y_pred = best_net.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "c_matrix = confusion_matrix(y_test, rnd_search_cv.predict(X_test[:len(y_test)]))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"DNN\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ca4fd-6ee2-4e76-b230-7aa5244e28e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d92dd4-98a3-4b37-983a-03d7251079de",
   "metadata": {},
   "source": [
    "## 5.0 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424eab6-ef4e-4c6b-a868-83d8a14b075b",
   "metadata": {},
   "source": [
    "Sorted by recall, the best models are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a801096-6a85-498e-8318-0bb616aaf74e",
   "metadata": {},
   "source": [
    "DNN performance using recall as the score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c8519d3-659e-4243-a44a-52da13655ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  Accuracy  Precision    Recall    F1\n",
       "0   DNN      0.55   0.666667  0.615385  0.64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9426c-4d70-4f44-9767-506166fb606a",
   "metadata": {},
   "source": [
    "Performance of the remaing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf2450cc-128f-4478-a026-6254143a8dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree_random</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree_grid</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN_GRID</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression rand search</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression rand search</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression grid search</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Random search</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM grid search</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN_Rand</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  Accuracy  Precision    Recall        F1\n",
       "0                     Dtree_random     0.550   0.681818  0.576923  0.625000\n",
       "0                       Dtree_grid     0.650   0.800000  0.615385  0.695652\n",
       "0                          NN_GRID     0.650   0.714286  0.769231  0.740741\n",
       "0  Logistic Regression rand search     0.650   0.700000  0.807692  0.750000\n",
       "0  Logistic Regression rand search     0.650   0.700000  0.807692  0.750000\n",
       "0  Logistic Regression grid search     0.650   0.700000  0.807692  0.750000\n",
       "0                SVM Random search     0.675   0.696970  0.884615  0.779661\n",
       "0                  SVM grid search     0.675   0.696970  0.884615  0.779661\n",
       "0                          NN_Rand     0.650   0.650000  1.000000  0.787879"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff4078-859f-4ca0-a7c6-e914a51a666d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b73ae-b461-4725-b6e3-412aae67215e",
   "metadata": {},
   "source": [
    "The dataset is connected to predicting the purchase of the Apple M1 Mac book. The purpose of this data set is to predict whether or not the customer will buy the M1 Macbook. As a result, the goal of this assignment is to establish which features you will employ to analyze purchase behaviors and how these features impact the sales of the apple mac book.\n",
    "\n",
    "I chose the recall performance indicator for this dataset because recall is used to quantify the proportion of true positives out of all possible outcomes.\n",
    "\n",
    "According to the above results, the neural network model with random search outperforms all four models with 100 percent accuracy, followed by SVM with random and grid search with 88.461 percent accuracy, logistic regression with random and grid search with 80.76 percent accuracy, deep neural network with approximately 62 percent, and decision tree with grid search with 61.53 percent accuracy.\n",
    "As a result, when recall is used as the performance metric, the Neural Network model surpasses the other models. \n",
    "\n",
    "As a result, when compared to neural networks with MLP classifiers and DNN with keras, we can conclude that the MLP classifier performs well, and the performance is also affected by various factors such as loss function, activation, and number of layers.\n",
    "\n",
    "The decision tree utilizing grid search model is shown to be the least performing model in terms of recall.\n",
    "When accuracy is used as a performance indicator, the decision tree model with grid search is deemed the best model, followed by logistic regression with random search and grid search, and SVM.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
